# Kafka Add On Module Release 0.3.0

Welcome to the latest release of `kafka` module of the [`SIGHUP Distribution`](https://github.com/sighupio/distribution) maintained with ‚ù§Ô∏è by team [SIGHUP by ReeVo](https://sighup.io/).

## Component Images üö¢

| Component                | Supported Version                                                                                      | Previous Version |
|--------------------------|--------------------------------------------------------------------------------------------------------|------------------|
| `Strimzi Operator`       | [`v0.45.0`](https://github.com/strimzi/strimzi-kafka-operator/releases/tag/0.45.0)                     | `v0.42.0`        |
| `Monitoring configs`     | `v0.3.0`                                                                                               | `v0.2.0`         |

> Please refer to the individual release notes to get detailed information on each release.

## Bug Fixes and Changes üêõ

- Strimzi operator upgraded to `v0.45.0`
- Add support for Kafka versions `3.8.0`, `3.8.1`, `3.9.0`

## Breaking Changes üíî

- Drop support for Kafka versions < `3.8.0`
- Drop support for ZooKeeper
- Drop support for Kafka cluster not using `KafkaNodePool`
- Switched to Kustomize v5.6.0

## Update Guide ü¶Æ

The upgrade procedure consists in four steps, two optional steps to prepare the cluster for the upgrade that need to be done only if the Kafka cluster being upgraded doesn't use `KRaft` and `KafkaNodePool`, and another two steps for the upgrade itself.

> [!NOTE]
> The first two steps can also be skipped if the user wants to continue using ZooKeeper, but we recommend migrating to KRaft. 

1. Migrate existing Kafka cluster to KafkaNodePool (Optional)
2. Migrate from ZooKeeper to KRaft (Optional)
3. Upgrade the operator.
4. Upgrade the Kafka cluster.


### 1. Migrate existing Kafka cluster to KafkaNodePool

If the already existing Kafka cluster(s) do not use `KafkaNodePool` then this step is necessary to migrate to `KafkaNodePool` before proceeding with the upgrade.

First, a `KafkaNodePool` resource needs to be created. The resource must be created with the following characteristics:

- It must be labeled with the `strimzi.io/cluster: <CLUSTER_NAME>` label where `<CLUSTER_NAME>` is the Kafka resource name.
- The replica count and storage configuration must match your current Kafka cluster.
- The roles must be set to `broker`.

Here is an example manifest:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: kafka
  labels:
    strimzi.io/cluster: my-cluster
spec:
  replicas: 3
  roles:
    - broker
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 10Gi
        deleteClaim: false
```

And here is an example Kafka resource manifest:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
  zookeeper:
    # ...
```

> [!WARNING]  
> To preserve cluster data and the names of its nodes and resources, the node pool name must be `kafka`, and the `strimzi.io/cluster` label must match the Kafka resource name. Otherwise, nodes and resources will be created with new names, including the persistent volume storage used by the nodes, and consequently your previous data may not be available.

Apply the KafkaNodePool resource with:

```bash
kubectl apply -f <node_pool_configuration_file>
```

Or if you are using Kustomize you can simply add the resource manifest file to the `resources` array. By applying this resource, you switch Kafka to use node pools (`KafkaNodePool`).

> [!NOTE]
> There is no change or rolling update and resources are identical to how they were before.

Secondly, enable support for node pools in the Kafka resource using the `strimzi.io/node-pools: enabled` annotation. Here is an example:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  annotations:
    strimzi.io/node-pools: enabled
spec:
  kafka:
    # ...
  zookeeper:
    # ...
```

Apply the Kafka resource.

> [!NOTE]
> There is no change or rolling update and resources are identical to how they were before.

Remove the replicated properties from the Kafka custom resource. When the `KafkaNodePool` resource is in use, you can remove the properties that you copied to the `KafkaNodePool` resource, such as the `.spec.kafka.replicas` and `.spec.kafka.storage` properties.

For more information about migrating to `KafkaNodePool` read the specific [documentation](https://strimzi.io/docs/operators/0.45.0/full/deploying#proc-migrating-clusters-node-pools-str)


### 2. Migrate from ZooKeeper to KRaft

ZooKeeper has been deprecated since Kafka version >= `3.5` and is set to be removed with version `4`. The new protocol is KRaft. In this step we will migrate an existing cluster which uses `KafkaNodePool` (necessary pre-requisite) from ZooKeeper to KRaft.

> [!IMPORTANT]  
> Be sure to have enough space for the provisioning of PVCs during the migration. For example, using a Kafka cluster with 3 brokers using 100Gi PVCs, a controller node pool with 3 replicas will need a PVC each of at least 10Gi. Bigger Kafka clusters may need more space for the controllers PVCs.

Edit the `Kafka` resource and add the `strimzi.io/kraft: disabled` annotation. Apply the change.

Create the `KafkaNodePool` resource for the KRaft controllers, here is an example:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: controller
  labels:
    strimzi.io/cluster: my-cluster
spec:
  replicas: 3
  roles:
    - controller
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 100Gi
        deleteClaim: false
```

Then change the annotation on the corresponding `Kafka` resource:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  annotations:
    strimzi.io/kraft: migration # updated from "disabled" to "migration" to trigger the migration process
    strimzi.io/node-pools: enabled
# ...
```

Apply the changes and wait for the operator to start the migration. Then monitor the Kafka resource with `kubectl get kafka my-cluster` and wait that it reaches the `KRaftPostMigration` state. Then there are two choices:

- Finalize the migration to KRaft.
- Rollback to ZooKeeper.

#### Finalize the migration to KRaft

To finalize the migration, the annotation `strimzi.io/kraft` needs to be changed to `enabled`, as follows:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  annotations:
    strimzi.io/kraft: enabled # updated from "migration" to "enabled" to finalize the migration process
    strimzi.io/node-pools: enabled
# ...
```

The operator will configure the KRaft controllers without the ZooKeeper connection details, with the migration flag disabled, and roll them. When both brokers and controllers are working in KRaft mode, the operator will delete all the resources related to ZooKeeper as well. At this point, the user should delete the `.spec.zookeeper` section from the Kafka custom resource because it is not needed anymore.

#### Rollback to ZooKeeper

If the migration process is still not finalized, with the KRaft controllers remaining connected to ZooKeeper, it is still possible to initiate a rollback. By applying the rollback value on the `strimzi.io/kraft` annotation, the operator reconfigures the brokers with the connection details to ZooKeeper again and rolls them. Here is an example:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  annotations:
    strimzi.io/kraft: rollback # updated from "migration" to "rollback" to revert the migration process
    strimzi.io/node-pools: enabled
# ...
```

Following the rollback, the cluster metadata state moves back to `KRaftDualWriting`, waiting for the user to finalize the process.
After that, delete the `KafkaNodePool` hosting the KRaft controllers and then apply the disabled value annotation.

### 3. Upgrade the operator

> [!NOTE]
> The upgrade triggers rolling updates, where brokers are restarted one by one at different stages of the process. During this time, overall cluster availability is temporarily reduced, which may increase the risk of message loss in the event of a broker failure.

Apply the new operator, the new deployment will start with the v0.45.0 running. If there are Kafka clusters of an unsupported version the operator will print an `unsupported version` warning in its logs. This will be resolved with the next step.

### 4. Upgrade the Kafka cluster

Edit the Kafka resource of the target cluster and change the version to `3.9.0` or another supported version (`3.8.0`, `3.8.1`, `3.9.0`). Also change the config section and add the following parameters:

If configured, check that the current `spec.kafka.metadataVersion` is set to a version supported by the version of Kafka you are upgrading to.

For example, the current version is 3.7‚ÄëIV4 if upgrading from Kafka version 3.7.1 to 3.9.0:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    replicas: 3
    metadataVersion: 3.7‚ÄëIV4
    version: 3.7.1
    # ...
```

Apply the modified Kafka resource. Wait for the operator to reconcile the cluster. Once done, check that everything is working correctly.

Change the `Kafka.spec.kafka.version` to specify the new Kafka version; leave the `metadataVersion` at the default for the current Kafka version. For example:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    replicas: 3
    metadataVersion: 3.7-IV4
    version: 3.9.0
    # ...
```

Apply the modified Kafka resource. Wait for the operator to reconcile the cluster. Once done, check that everything is working correctly.

If present update KafkaConnect and KafkaMirrorMaker versions.

#### Rolling back the upgrade

In case of errors the upgrade can be rolled back downgrading the operator to the previous version and undoing the changes to the Kafka resource.

#### Finalizing the upgrade

If everything works correctly, the upgrade can be finalized by updating the Kafka resource to use the new `metadataVersion` version, for example:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    replicas: 3
    metadataVersion: 3.9-IV0
    version: 3.9.0
    # ...
```

> [!WARNING]
> Exercise caution when changing the metadataVersion, as downgrading may not be possible. You cannot downgrade Kafka if the metadataVersion for the new Kafka version is higher than the Kafka version you wish to downgrade to.

Wait for the operator to reconcile the cluster and then the upgrade is concluded.

For more information about the upgrade procedure please read Kafka upgrade [documentation](https://kafka.apache.org/39/documentation.html#upgrade). Also useful information can be found on the Strimzi operator [documentation](https://strimzi.io/docs/operators/0.45.0/full/deploying#proc-upgrade-kafka-kraft-str).

## Package Strimzi operator! üì¶

Strimzi provides a way to run an Apache Kafka¬Æ cluster on Kubernetes or OpenShift in various deployment configurations.

Our package will be installed as a cluster wide operator, managing clusters on all namespaces.

## Package Monitoring configs! üì¶

This package contains the required configurations to monitor the operator and the Kafka clusters using Prometheus and Grafana.
It is based on the [SIGHUP Monitoring][module-monitoring] module.

## Package deployment üöÄ

To install all the packages, see the [deployment section](../../README.md#deployment) of the main README.

<!-- Links -->

[module-monitoring]: https://github.com/sighupio/module-monitoring





